{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Treinamento de usuário logado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carregando Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_csv_path = \"../artifacts/parquets\"\n",
    "user_target_csv_file = \"merged_data.parquet\"\n",
    "\n",
    "articles_csv_path = \"../artifacts/parquets\"\n",
    "articles_target_csv_file = \"articles_merged_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_parquet(f\"{user_csv_path}/{user_target_csv_file}\")\n",
    "noticias_df = pd.read_parquet(f\"{articles_csv_path}/{articles_target_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecionando usuários logados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_users_df = users_df[users_df['userType'] == 'Logged']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre processamento notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.data.clear_cache()\n",
    "# nltk_data_path = \"/home/heijimor/Desktop/fiap/datathon/train_service/artifacts/download/\"  # ou outro caminho, se necessário\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# nltk.download('all', download_dir=nltk_data_path)\n",
    "stop_words = set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "noticias_df['processed_body'] = noticias_df['body'].apply(preprocess_text)\n",
    "print(noticias_df[['title', 'processed_body']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(clicks_str):\n",
    "    clicks = list(map(int, clicks_str.split(',')))\n",
    "    return sum(clicks)\n",
    "\n",
    "logged_users_df['numberOfClicksHistory'] = logged_users_df['numberOfClicksHistory'].apply(convert_to_numeric)\n",
    "logged_users_df['timeOnPageHistory'] = logged_users_df['timeOnPageHistory'].apply(convert_to_numeric)\n",
    "logged_users_df['scrollPercentageHistory'] = logged_users_df['scrollPercentageHistory'].apply(\n",
    "    lambda x: sum(map(float, x.split(','))) if isinstance(x, str) else 0\n",
    ")\n",
    "logged_users_df['pageVisitsCountHistory'] = logged_users_df['pageVisitsCountHistory'].apply(convert_to_numeric)\n",
    "\n",
    "# # Normalizar os dados de comportamento do usuário e calcular o índice de engajamento\n",
    "logged_users_df['engagement'] = (\n",
    "    logged_users_df['numberOfClicksHistory'] * 0.2 + # Peso para cliques\n",
    "    logged_users_df['timeOnPageHistory'] * 0.3 +      # Peso para tempo na página\n",
    "    logged_users_df['scrollPercentageHistory'] * 0.2 + # Peso para scroll\n",
    "    logged_users_df['pageVisitsCountHistory'] * 0.3    # Peso para visitas\n",
    ")\n",
    "\n",
    "# # Verificar o novo índice de engajamento\n",
    "print(logged_users_df[['userId', 'engagement']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Passo 2: Similaridade de conteúdo (TF-IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words_pt = stopwords.words('portuguese')\n",
    "\n",
    "# Usar o TF-IDF para calcular a similaridade entre os corpos das notícias\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,  # Remove termos muito comuns\n",
    "    min_df=1,  # Remove termos muito raros\n",
    "    max_features=10000,  # Aumenta o limite de palavras únicas\n",
    "    stop_words=stop_words_pt  # Aqui passamos a lista correta!\n",
    ")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(noticias_df['processed_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Reduzindo a dimensionalidade da matriz TF-IDF\n",
    "sample = 0.2\n",
    "svd = TruncatedSVD(n_components=100)  # Ajuste o número de componentes conforme necessário\n",
    "sampled_indices = np.random.choice(tfidf_matrix.shape[0], size=int(tfidf_matrix.shape[0] * sample), replace=False)  # Amostra 20% das linhas\n",
    "sampled_tfidf_matrix = tfidf_matrix[:int(tfidf_matrix.shape[0] * sample)]\n",
    "reduced_tfidf = svd.fit_transform(sampled_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(sampled_tfidf_matrix)\n",
    "lda_topics = lda.transform(sampled_tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140780    1.0\n",
      "160628    2.0\n",
      "248329    0.0\n",
      "40484     0.0\n",
      "101704    0.0\n",
      "Name: category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "noticias_df['category'].fillna(0.0, inplace=True)\n",
    "noticias_df.loc[sampled_indices, 'category'] = np.argmax(lda_topics, axis=1)\n",
    "print(noticias_df.loc[sampled_indices, 'category'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exibindo as 10 palavras mais relevantes para cada tópico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 10\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    print([tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calcular a similaridade entre as notícias usando a matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "knn.fit(reduced_tfidf)\n",
    "distances, indices = knn.kneighbors(reduced_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "artifacts_models_path = \"../artifacts/models\"\n",
    "\n",
    "with open(f\"{artifacts_models_path}/knn_model.pkl\", \"wb\") as f:\n",
    "  pickle.dump(knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajuste da Similaridade com o Perfil do Usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajusta_recomendacao(user_profile, similarity_matrix):\n",
    "    # O perfil do usuário pode ser uma combinação de suas preferências (ex: mais tempo em esportes, etc.)\n",
    "    # Por exemplo, se o usuário tem maior engajamento com esportes, aumenta a similaridade das notícias de esportes\n",
    "    ajusted_similarity = similarity_matrix * user_profile\n",
    "    return ajusted_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criar a função de recomendação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_news_knn(user_id, top_n=5):\n",
    "    # print(noticias_df.loc[sampled_indices, 'category'].head())\n",
    "    # Obter o comportamento do usuário logado\n",
    "    user_data = logged_users_df[logged_users_df['userId'] == user_id]\n",
    "    \n",
    "    if user_data.empty:\n",
    "        print(\"Usuário não encontrado.\")\n",
    "        return\n",
    "\n",
    "    user_engagement = user_data['engagement'].values[0] # Encontrar o conteúdo mais relevante com base no engajamento\n",
    "    user_history = user_data['history'].values[0] # Determinar as categorias mais consumidas pelo usuário\n",
    "    # ISSO PRECISA SER PRE PROCESSADO DEPOIS\n",
    "    user_history = user_history.split(\", \")\n",
    "\n",
    "    if not user_history:\n",
    "        print(\"Nenhum histórico encontrado para o usuário.\")\n",
    "        return None\n",
    "\n",
    "    sampled_history = [str(article) for article in user_history if str(article) in noticias_df['page'].astype(str).values]\n",
    "    print(sampled_history)\n",
    "    # Obter categorias baseadas nos artigos do histórico do usuário\n",
    "    sample_news_pages = noticias_df.loc[sampled_indices, 'page'].astype(str)\n",
    "    sample_news_pages = sample_news_pages.str.strip()\n",
    "\n",
    "    print(sample_news_pages)\n",
    "    # Verificar se as páginas amostradas estão na lista de histórico do usuário\n",
    "    sample_page_id = sample_news_pages.isin(map(str, sampled_history))\n",
    "    print(sample_page_id)\n",
    "    # print(sample_page_id)\n",
    "    print(sample_page_id.sum())  # Verificar quantos valores são True\n",
    "    # print(noticias_df['category'].isnull().sum())  # Verificar se há valores nulos\n",
    "    # # print(noticias_df['category'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['385044ad-3876-4188-83fa-f560435c1a9c', '2f754502-7014-488b-88c7-d41328d5e80d']\n",
      "140780    c8a69d1a-206f-4ae1-84f0-e237832d93be\n",
      "160628    365df623-85b5-4829-820c-3467930abfaa\n",
      "248329    87179350-0b63-4b94-9b9d-b90b6bbef042\n",
      "40484     94595cd8-ee10-49db-890a-22f15e9c6248\n",
      "101704    8e101739-3f34-4af0-9b71-fb3e35d94e56\n",
      "                          ...                 \n",
      "252243    67c6ee47-7f2b-489f-9376-25ce2bb9adab\n",
      "176825    81be96bd-9d7f-4033-a329-aa4cc6a540bb\n",
      "57003     8e866c58-dd0b-4ee7-b7d6-4b5d8f7464cc\n",
      "228911    d3eafc57-f82e-4c1d-bf59-03e0948092e4\n",
      "146105    0861cdab-70b2-4605-9598-b139d2b81825\n",
      "Name: page, Length: 51120, dtype: object\n",
      "140780    False\n",
      "160628    False\n",
      "248329    False\n",
      "40484     False\n",
      "101704    False\n",
      "          ...  \n",
      "252243    False\n",
      "176825    False\n",
      "57003     False\n",
      "228911    False\n",
      "146105    False\n",
      "Name: page, Length: 51120, dtype: bool\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "recommended_news_knn = recommend_news_knn(user_id='97e1439d485b0630e12818d3df84ff67d08475ef6ebeb08e5354779ad6a30fdb', top_n=6)\n",
    "# print(recommended_news_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avaliação e Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos calcular uma métrica simples de precisão para avaliar o modelo\n",
    "# Assumindo que temos uma coluna 'liked_news' no dataframe 'logged_users_df' que indica quais notícias o usuário curtiu\n",
    "\n",
    "# Simular a coluna 'liked_news' para avaliação (deve ser substituída por dados reais)\n",
    "logged_users_df['liked_news'] = logged_users_df['userId'].apply(lambda x: noticias_df['title'].sample(1).values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avaliar a precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(user_id, top_n=5):\n",
    "    recommended_news = recommend_news_knn(user_id, top_n)\n",
    "    print(f\"recommended_news: {recommended_news}\")\n",
    "    if recommended_news is not None:\n",
    "        liked_news = logged_users_df[logged_users_df['userId'] == user_id]['liked_news'].values[0]\n",
    "        print(f\"liked_news: {liked_news}\")\n",
    "        # Verificar se alguma das notícias recomendadas é a mesma que o usuário gostou\n",
    "        hits = recommended_news[recommended_news['title'].isin([liked_news])].shape[0]\n",
    "        print(f\"hits: {hits}\")\n",
    "        precision = hits / top_n  # Precisão (fração de acertos nas top_n notícias recomendadas)\n",
    "        print(f\"precision: {precision}\")\n",
    "        return precision\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "precision = evaluate_recommendations(user_id='97e1439d485b0630e12818d3df84ff67d08475ef6ebeb08e5354779ad6a30fdb', top_n=6)\n",
    "print(f'Precisão da recomendação: {precision * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
